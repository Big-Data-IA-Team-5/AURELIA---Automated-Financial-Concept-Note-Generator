import chromadb
from chromadb.config import Settings
import openai
from typing import List, Dict
import os
from sentence_transformers import SentenceTransformer

class VectorRetrievalService:
    def __init__(self):
        # Use sentence transformers for embeddings (faster than OpenAI for local)
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        
        # Initialize ChromaDB
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.collection_name = "aurelia_financial"
        
        # Try to get existing collection or create new one
        try:
            self.collection = self.client.get_collection(self.collection_name)
        except:
            self.collection = self.client.create_collection(self.collection_name)
            self._seed_basic_content()
    
    def _seed_basic_content(self):
        """Seed with basic financial concept content"""
        financial_concepts = [
            {
                "id": "duration_1",
                "content": "Duration measures the price sensitivity of a bond to changes in interest rates. Modified duration adjusts Macaulay duration for yield changes.",
                "metadata": {"concept": "duration", "page_num": 15, "source": "fintbx"}
            },
            {
                "id": "sharpe_1", 
                "content": "The Sharpe ratio measures risk-adjusted return by dividing excess return by standard deviation. Higher ratios indicate better risk-adjusted performance.",
                "metadata": {"concept": "sharpe ratio", "page_num": 23, "source": "fintbx"}
            },
            {
                "id": "beta_1",
                "content": "Beta measures systematic risk by comparing asset returns to market returns. Beta of 1 means the asset moves with the market.",
                "metadata": {"concept": "beta", "page_num": 31, "source": "fintbx"}
            },
            {
                "id": "var_1",
                "content": "Value at Risk (VaR) estimates potential losses over a specific time period with given confidence level. Common confidence levels are 95% and 99%.",
                "metadata": {"concept": "var", "page_num": 45, "source": "fintbx"}
            },
            {
                "id": "alpha_1",
                "content": "Alpha measures excess returns generated by an investment relative to a benchmark. Positive alpha indicates outperformance.",
                "metadata": {"concept": "alpha", "page_num": 38, "source": "fintbx"}
            }
        ]
        
        documents = [item["content"] for item in financial_concepts]
        metadatas = [item["metadata"] for item in financial_concepts]
        ids = [item["id"] for item in financial_concepts]
        
        self.collection.add(
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        print(f"Seeded {len(financial_concepts)} financial concepts to vector store")
    
    def search_similar(self, query: str, n_results: int = 5) -> Dict:
        """Search for similar chunks with real embeddings"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=min(n_results, self.collection.count())
            )
            
            if not results['documents'][0]:
                return {
                    "chunks": [],
                    "scores": [],
                    "total_found": 0
                }
            
            chunks = []
            for i, doc in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                distance = results['distances'][0][i] if results['distances'] else 1.0
                score = max(0, 1 - distance)  # Convert distance to similarity score
                
                chunks.append({
                    "content": doc,
                    "metadata": metadata,
                    "score": score,
                    "page_num": metadata.get('page_num', 0)
                })
            
            return {
                "chunks": chunks,
                "scores": [chunk["score"] for chunk in chunks],
                "total_found": len(chunks)
            }
            
        except Exception as e:
            print(f"Vector search error: {e}")
            return {"chunks": [], "scores": [], "total_found": 0}

# Global instance
retrieval_service = VectorRetrievalService()